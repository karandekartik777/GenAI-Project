# -*- coding: utf-8 -*-
"""website project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-u7ZkgH0QXzhK48H4DCuZ-04h4WrVAro
"""

!pip -q install langchain
!pip -q install bitsandbytes accelerate transformers
!pip -q install datasets loralib sentencepiece
!pip -q install pypdf
!pip -q install sentence_transformers

!pip -q install unstructured

!pip install tokenizers
!pip install xformers
!pip install pinecone-client
!pip install langchain_community

!pip install pinecone-client
# from langchain.document_loaders import UnstructuredURLLoader
from langchain_community.document_loaders import UnstructuredURLLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.vectorstores import Pinecone
from langchain_community.vectorstores import Chroma

from langchain.chains import RetrievalQAWithSourcesChain
from langchain.embeddings import HuggingFaceEmbeddings
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import pipeline
from langchain import HuggingFacePipeline
from huggingface_hub import notebook_login
import textwrap
import sys
import os
import torch

URLs=[
    'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb',
    'https://www.mosaicml.com/blog/mpt-7b',
    'https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models',
    'https://lmsys.org/blog/2023-03-30-vicuna/'

]


import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

loaders = UnstructuredURLLoader(urls=URLs)
data = loaders.load()

data

len(data)

type(data)

from langchain.text_splitter import RecursiveCharacterTextSplitter

def split_docs(documents,chunk_size=1000,chunk_overlap=20):
  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
  docs = text_splitter.split_documents(documents)
  return docs

docs = split_docs(data)
print(len(docs))

from langchain.embeddings import SentenceTransformerEmbeddings
embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

!pip install chromadb
from langchain.vectorstores import Chroma
db = Chroma.from_documents(docs, embeddings)





embeddings = HuggingFaceEmbeddings()

query = "What is open ai"
matching_docs = db.similarity_search(query)

matching_docs[0]

persist_directory = "chroma_db"

vectordb = Chroma.from_documents(
    documents=docs, embedding=embeddings, persist_directory=persist_directory
)

vectordb.persist()



notebook_login()

model = 'meta-llama/Llama-2-7b-chat-hf'

tokenizer = AutoTokenizer.from_pretrained(model,use_auth_toke = True)

model = AutoModelForCausalLM.from_pretrained(model,device_map = 'auto',
                                             torch_dtype = torch.float16,use_auth_token = True,
                                             load_in_8bit = True)

!nvidia-smi

pipe = pipeline("text-generation",
                model=model,
                tokenizer= tokenizer,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                max_new_tokens = 512,
                do_sample=True,
                top_k=30,
                num_return_sequences=1,
                eos_token_id=tokenizer.eos_token_id
                )

from langchain.chains import RetrievalQA

llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})

llm.predict("what is Machine Learning in AI")

query = "How good is open ai models than Vicuna?"

vectordb = vectordb.similarity_search(query, k=3)




langchainChroma = Chroma(persist_directory=persist_directory, embedding_function=embeddings)
retriver = langchainChroma.as_retriever(search_kwargs={"k":2})
qa = RetrievalQA.from_chain_type(llm = llm, chain_type='stuff', retriever = retriver)

query = "How good is Vicuna? model than mistral "
qa.run(query)

